{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6IjIda+PAjPCkbNWKkdnb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btoneil2021/Search-Minecraft-Wiki/blob/main/MinecraftWikiScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDH7j7ZgiyLT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "HOW TO USE:\n",
        "1. Type API key below\n",
        "2. Run\n",
        "'''\n",
        "api_key = \"[INSERT YOUR API KEY HERE]\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 (and 2 I guess). Scrape Data off Minecraft Wiki"
      ],
      "metadata": {
        "id": "ivhm8ZLsjQAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "pip install chromedriver_autoinstaller\n",
        "pip install selenium\n",
        "pip install beautifulsoup4\n",
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgLU7xGtrfIH",
        "outputId": "d09a6f0a-878f-4417-dd3d-3547a7f7fb5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromedriver_autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from chromedriver_autoinstaller) (24.1)\n",
            "Installing collected packages: chromedriver_autoinstaller\n",
            "Successfully installed chromedriver_autoinstaller-0.6.4\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.0-py3-none-any.whl (475 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 trio-0.26.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import chromedriver_autoinstaller\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import threading\n",
        "import asyncio"
      ],
      "metadata": {
        "id": "0OFVW-6fjfdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_soup(driver, url):\n",
        "  driver.get(url)\n",
        "\n",
        "  driver.implicitly_wait(1)\n",
        "\n",
        "  soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "  return soup"
      ],
      "metadata": {
        "id": "gtFcpiSajkP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Options\n",
        "# 1. The page is found and is automatically opened\n",
        "# 2. The page is not found due to some kind of error and the first link in the search is taken\n",
        "def search_for_page(driver, search_term):\n",
        "  '''\n",
        "  input: search_term - (string) what you're looking for\n",
        "         driver - (webdriver) the Selenium webdriver instance\n",
        "  output: url - (string) the url of the page of results you're looking for\n",
        "  '''\n",
        "  # I'm just going to be lazy and add an \"i\" at the end of everything so everything gets kicked to the search page by default\n",
        "  url = f'https://minecraft.wiki/w/Special:Search?search={search_term.replace(\" \", \"+\")}i'\n",
        "\n",
        "  soup = get_soup(driver, url)\n",
        "  url_addition_tag = soup.find(\"a\", {\"data-serp-pos\": \"0\"})\n",
        "  if url_addition_tag and url_addition_tag.has_attr(\"href\"):\n",
        "      url_addition = url_addition_tag[\"href\"]\n",
        "      url = f'https://minecraft.wiki{url_addition}'\n",
        "  else:\n",
        "      url = driver.current_url \n",
        "\n",
        "  return url"
      ],
      "metadata": {
        "id": "HiA_Jls0kXch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_info_box(soup):\n",
        "  info_box = soup.find(\"div\", {\"class\": \"notaninfobox\"})\n",
        "  info = {}\n",
        "  if not info_box:\n",
        "    return info # Return empty if no info_box\n",
        "\n",
        "  # Start by Grabbing all the top-of-table information\n",
        "  name_tag = info_box.find(\"div\", {\"class\": \"mcwiki-header infobox-title\"})\n",
        "  if name_tag:\n",
        "    info[\"name\"] = name_tag.text.strip()\n",
        "  \n",
        "  image_tag_container = info_box.find(\"div\", {\"class\": \"infobox-imagearea animated-container\"})\n",
        "  if image_tag_container:\n",
        "    image_tag = image_tag_container.find(\"img\")\n",
        "    if image_tag and image_tag.has_attr('src'):\n",
        "      info[\"image_url\"] = \"https://minecraft.wiki\" + image_tag[\"src\"]\n",
        "\n",
        "  # Ok now we can grab the info from each table section\n",
        "  info[\"details\"] = {}\n",
        "  table = info_box.find(\"table\", {\"class\": \"infobox-rows\"})\n",
        "  if table:\n",
        "    table_rows = table.find_all(\"tr\")\n",
        "    for row in table_rows:\n",
        "      key = row.find(\"th\")\n",
        "      if key is None:\n",
        "        continue\n",
        "      value = row.find(\"td\")\n",
        "      if value is None:\n",
        "        continue\n",
        "      info[\"details\"][key.text.strip()] = value.text.strip()\n",
        "\n",
        "  return info"
      ],
      "metadata": {
        "id": "m-ObjS4--hcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_main_paragraph(soup):\n",
        "  main_paragraph_container = soup.find(\"div\", {\"class\": \"mw-body-content mw-content-ltr\"})\n",
        "  if not main_paragraph_container:\n",
        "    return [] \n",
        "  \n",
        "  main_paragraph_elements = main_paragraph_container.find_all([\"p\", \"h2\", \"h3\"])\n",
        "\n",
        "  tables = soup.find_all(\"table\")\n",
        "  paragraphs_in_tables = []\n",
        "  for table in tables:\n",
        "    paragraphs_in_tables.extend(table.find_all(\"p\"))\n",
        "  \n",
        "  paragraphs_in_tables_set = set(paragraphs_in_tables)\n",
        "\n",
        "  # Clearer logic for filtering, ensuring h2/h3 are always included\n",
        "  filtered = [el for el in main_paragraph_elements if el.name in ['h2', 'h3'] or (el.name == 'p' and el not in paragraphs_in_tables_set)]\n",
        "  main_paragraph = [el.text.strip() for el in filtered]\n",
        "  return main_paragraph"
      ],
      "metadata": {
        "id": "TM-fb9B7JAIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new_scraping_cell"
      },
      "outputs": [],
      "source": [
        "def execute_scraping(search_subject, chrome_options):\n",
        "  driver = webdriver.Chrome(options=chrome_options)\n",
        "  driver.implicitly_wait(1)\n",
        "  try:\n",
        "    content_url = search_for_page(driver, search_subject)\n",
        "    soup = get_soup(driver, content_url)\n",
        "    return soup, driver\n",
        "  except Exception as e:\n",
        "    print(f\"Error during scraping: {e}\")\n",
        "    if driver:\n",
        "        driver.quit()\n",
        "    return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Throw Everything into ChatGPT"
      ],
      "metadata": {
        "id": "uqaA6xCZQP39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "NGUB8_qiQVXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447f773c-281f-4b70-e86b-129f18c539f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.14-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: httpcore, httpx, openai\n",
            "Successfully installed httpcore-1.0.5 httpx-0.27.0 openai-1.35.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AsyncOpenAI\n",
        "client = AsyncOpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "PH9LtoSMvpNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def make_chatgpt_request(prompt_text):\n",
        "  chat_completion = await client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt_text\n",
        "          }\n",
        "      ]\n",
        "  )\n",
        "  return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "r9r5Q9XGxE5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_feature_extraction = '''You are an AI designed to assist with extracting specific features from user requests related to the game Minecraft. Your task is to analyze the user query (delimited by triple backticks) and extract two key pieces of information:\n",
        "\n",
        "Subject: Identify the main subject of the sentence. This is typically a Minecraft item, entity, block, or structure (full names only).\n",
        "Summary: Provide a brief summary of the specific information or request that the user wants to know about the subject. Include any information that may be important.\n",
        "Here are some examples to guide you:\n",
        "\n",
        "Example 1:\n",
        "User query: \"How do I find diamonds in Minecraft?\n",
        "Subject: \"diamonds\"\n",
        "Summary: \"How to find\"\n",
        "\n",
        "Example 2:\n",
        "User query: \"What are the best enchantments for a sword?\"\n",
        "Subject: \"sword\"\n",
        "Summary: \"best enchantments\"\n",
        "\n",
        "When processing each query, please answer with only the subject and the summary, separated by a backslash.\n",
        "\n",
        "```\n",
        "\n",
        "'''\n",
        "\n",
        "generate_output = '''You are an AI designed to extract specific answers from provided text based on a given summary. Your task is to read the given paragraphs and answer the summary as accurately as possible using the information from the text.\n",
        "Do not change anything in the summary.\n",
        "\n",
        "Here is the format you should follow:\n",
        "\n",
        "Summary: This is a brief statement or question that you need to answer.\n",
        "Paragraphs: These are the detailed texts from which you will extract the necessary information to answer the summary.\n",
        "Answer: This is the accurate and concise response to the summary based on the provided paragraphs.\n",
        "Use the following example as a guide:\n",
        "\n",
        "Example:\n",
        "Summary: benefits of a beacon\n",
        "Paragraphs: Beacons are powerful items in Minecraft that provide various status effects to players within a certain radius. When activated, beacons can grant players speed, haste, resistance, jump boost, or strength. Additionally, a fully powered beacon can give regeneration or a secondary status effect. Beacons are often used in bases to enhance player abilities and provide strategic advantages during combat or resource gathering.\n",
        "Answer: Beacons provide status effects such as speed, haste, resistance, jump boost, and strength. Fully powered beacons can also give regeneration or a secondary effect, enhancing player abilities and providing strategic advantages in combat and resource gathering.\n",
        "\n",
        "When processing each query, please answer with only the answer to the summary.\n",
        "\n",
        "The summary and paragraphs (respectively) are defined below, delimited by triple backticks.\n",
        "```\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "KM4r-XTxxPFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def prompt(user_query):\n",
        "  # api_key is global, client (AsyncOpenAI) should be initialized globally or passed if not using global\n",
        "  \n",
        "  feature_extraction_prompt = basic_feature_extraction + user_query + \"\\n```\"\n",
        "  summary_response_text = await make_chatgpt_request(feature_extraction_prompt)\n",
        "  summary_parts = summary_response_text.strip(\"\\'\\\"\").split(\"\\n\")\n",
        "    \n",
        "  if len(summary_parts) < 2:\n",
        "      print(f\"Unexpected summary format: {summary_parts}\")\n",
        "      subject = user_query \n",
        "      summary_query = \"General information\"\n",
        "  else:\n",
        "      subject = summary_parts[0].strip(\"Subject: \").strip('\"')\n",
        "      summary_query = summary_parts[1].strip(\"Summary: \").strip('\"')\n",
        "\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  chrome_options.add_argument('--disable-gpu')\n",
        "\n",
        "  driver = None\n",
        "  soup_obj = None\n",
        "  info_results = {}\n",
        "  paragraphs_results = []\n",
        "\n",
        "  try:\n",
        "    soup_obj, driver = await asyncio.to_thread(execute_scraping, subject, chrome_options)\n",
        "    \n",
        "    if soup_obj:\n",
        "        info_task = asyncio.to_thread(extract_info_box, soup_obj)\n",
        "        paragraphs_task = asyncio.to_thread(extract_main_paragraph, soup_obj)\n",
        "        info_results, paragraphs_results = await asyncio.gather(info_task, paragraphs_task)\n",
        "    else:\n",
        "        print(f\"Warning: Scraping failed for subject '{subject}', no soup object returned.\")\n",
        "        # Defaults are already set for info_results, paragraphs_results\n",
        "\n",
        "    info_text_parts = []\n",
        "    if 'name' in info_results:\n",
        "        info_text_parts.append(f\"Name: {info_results['name']}\")\n",
        "    if 'details' in info_results and isinstance(info_results['details'], dict):\n",
        "        for k, v in info_results['details'].items():\n",
        "            info_text_parts.append(f\"{k}: {v}\")\n",
        "    info_text = \"\\n\".join(info_text_parts)\n",
        "    \n",
        "    paragraphs_text = \"\\n\".join(paragraphs_results)\n",
        "    \n",
        "    scraped_content = f\"{info_text}\\n{paragraphs_text}\".strip()\n",
        "\n",
        "    if not scraped_content:\n",
        "        print(f\"Warning: No information extracted from info_box or paragraphs for subject '{subject}'.\")\n",
        "        return f\"Could not extract detailed information for '{subject}' regarding '{summary_query}'. The wiki page might not have the expected structure or the content is missing.\"\n",
        "\n",
        "    final_gpt_prompt = generate_output + summary_query + \"\\n```\\n\" + scraped_content + \"\\n```\"\n",
        "    final_answer = await make_chatgpt_request(final_gpt_prompt)\n",
        "    return final_answer\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred in prompt: {e}\")\n",
        "    return \"Sorry, I encountered an error processing your request.\"\n",
        "  finally:\n",
        "    if driver:\n",
        "        await asyncio.to_thread(driver.quit)"
      ],
      "metadata": {
        "id": "zaMdD7Du07fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "  response = await prompt(input(\"What would you like to ask? \"))\n",
        "  print(response)"
      ],
      "metadata": {
        "id": "Ur84CyC81ar5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfJzW196hVJ-",
        "outputId": "8d888080-f09a-48f7-e314-5065b9ec703c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What would you like to ask? What is dirt?\n",
            "Summary: What is dirt used for in Minecraft?\n",
            "\n",
            "Answer: Dirt is primarily used for farming in Minecraft, but due to its abundance, it can also be used as a readily available building block.\n"
          ]
        }
      ]
    }
  ]
}